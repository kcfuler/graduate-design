This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.gradio/
  certificate.pem
app/
  exporters/
    csv.py
  metrics/
    collector.py
  models/
    __init__.py
    base.py
    mobilenet.py
  processors/
    image.py
  main.py
tests/
  test_app.py
  test_exporters.py
  test_metrics.py
  test_models.py
  test_processors.py
.cursorrules
README.md
requirements.txt
traffic_sign_recognition.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".gradio/certificate.pem">
-----BEGIN CERTIFICATE-----
MIIFazCCA1OgAwIBAgIRAIIQz7DSQONZRGPgu2OCiwAwDQYJKoZIhvcNAQELBQAw
TzELMAkGA1UEBhMCVVMxKTAnBgNVBAoTIEludGVybmV0IFNlY3VyaXR5IFJlc2Vh
cmNoIEdyb3VwMRUwEwYDVQQDEwxJU1JHIFJvb3QgWDEwHhcNMTUwNjA0MTEwNDM4
WhcNMzUwNjA0MTEwNDM4WjBPMQswCQYDVQQGEwJVUzEpMCcGA1UEChMgSW50ZXJu
ZXQgU2VjdXJpdHkgUmVzZWFyY2ggR3JvdXAxFTATBgNVBAMTDElTUkcgUm9vdCBY
MTCCAiIwDQYJKoZIhvcNAQEBBQADggIPADCCAgoCggIBAK3oJHP0FDfzm54rVygc
h77ct984kIxuPOZXoHj3dcKi/vVqbvYATyjb3miGbESTtrFj/RQSa78f0uoxmyF+
0TM8ukj13Xnfs7j/EvEhmkvBioZxaUpmZmyPfjxwv60pIgbz5MDmgK7iS4+3mX6U
A5/TR5d8mUgjU+g4rk8Kb4Mu0UlXjIB0ttov0DiNewNwIRt18jA8+o+u3dpjq+sW
T8KOEUt+zwvo/7V3LvSye0rgTBIlDHCNAymg4VMk7BPZ7hm/ELNKjD+Jo2FR3qyH
B5T0Y3HsLuJvW5iB4YlcNHlsdu87kGJ55tukmi8mxdAQ4Q7e2RCOFvu396j3x+UC
B5iPNgiV5+I3lg02dZ77DnKxHZu8A/lJBdiB3QW0KtZB6awBdpUKD9jf1b0SHzUv
KBds0pjBqAlkd25HN7rOrFleaJ1/ctaJxQZBKT5ZPt0m9STJEadao0xAH0ahmbWn
OlFuhjuefXKnEgV4We0+UXgVCwOPjdAvBbI+e0ocS3MFEvzG6uBQE3xDk3SzynTn
jh8BCNAw1FtxNrQHusEwMFxIt4I7mKZ9YIqioymCzLq9gwQbooMDQaHWBfEbwrbw
qHyGO0aoSCqI3Haadr8faqU9GY/rOPNk3sgrDQoo//fb4hVC1CLQJ13hef4Y53CI
rU7m2Ys6xt0nUW7/vGT1M0NPAgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBBjAPBgNV
HRMBAf8EBTADAQH/MB0GA1UdDgQWBBR5tFnme7bl5AFzgAiIyBpY9umbbjANBgkq
hkiG9w0BAQsFAAOCAgEAVR9YqbyyqFDQDLHYGmkgJykIrGF1XIpu+ILlaS/V9lZL
ubhzEFnTIZd+50xx+7LSYK05qAvqFyFWhfFQDlnrzuBZ6brJFe+GnY+EgPbk6ZGQ
3BebYhtF8GaV0nxvwuo77x/Py9auJ/GpsMiu/X1+mvoiBOv/2X/qkSsisRcOj/KK
NFtY2PwByVS5uCbMiogziUwthDyC3+6WVwW6LLv3xLfHTjuCvjHIInNzktHCgKQ5
ORAzI4JMPJ+GslWYHb4phowim57iaztXOoJwTdwJx4nLCgdNbOhdjsnvzqvHu7Ur
TkXWStAmzOVyyghqpZXjFaH3pO3JLF+l+/+sKAIuvtd7u+Nxe5AW0wdeRlN8NwdC
jNPElpzVmbUq4JUagEiuTDkHzsxHpFKVK7q4+63SM1N95R1NbdWhscdCb+ZAJzVc
oyi3B43njTOQ5yOf+1CceWxG1bQVs5ZufpsMljq4Ui0/1lvh+wjChP4kqKOJ2qxq
4RgqsahDYVvTH9w7jXbyLeiNdd8XM2w9U/t7y0Ff/9yi0GE44Za4rF2LN9d11TPA
mRGunUHBcnWEvgJBQl9nJEiU0Zsnvgc/ubhPgXRR4Xq37Z0j4r7g1SgEEzwxA57d
emyPxgcYxn/eR44/KJ4EBs+lVDR3veyJm+kXQ99b21/+jh5Xos1AnX5iItreGCc=
-----END CERTIFICATE-----
</file>

<file path="app/exporters/csv.py">
import os
import pandas as pd
from typing import List, Dict, Any, Optional
from datetime import datetime


class CSVExporter:
    """CSV 数据导出器"""
    
    def __init__(self, output_dir: str = "exports"):
        """
        初始化导出器
        
        Args:
            output_dir: 输出目录路径
        """
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
    
    def _generate_filename(self, prefix: str = "export") -> str:
        """
        生成导出文件名
        
        Args:
            prefix: 文件名前缀
            
        Returns:
            完整的文件路径
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{prefix}_{timestamp}.csv"
        return os.path.join(self.output_dir, filename)
    
    def export_inference_results(
        self,
        results: List[Dict[str, Any]],
        filename: Optional[str] = None
    ) -> str:
        """
        导出推理结果
        
        Args:
            results: 推理结果列表
            filename: 自定义文件名（可选）
            
        Returns:
            导出的文件路径
        """
        if not results:
            raise ValueError("导出数据不能为空")
        
        # 准备数据
        data = []
        for result in results:
            row = {
                "timestamp": datetime.now().isoformat(),
                "class_id": result.get("class_id"),
                "class_name": result.get("class_name"),
                "confidence": result.get("confidence")
            }
            
            # 添加额外的元数据
            if "metadata" in result:
                row.update(result["metadata"])
            
            data.append(row)
        
        # 创建 DataFrame
        df = pd.DataFrame(data)
        
        # 确定输出文件路径
        output_path = filename if filename else self._generate_filename("inference")
        
        # 导出到 CSV
        df.to_csv(output_path, index=False)
        return output_path
    
    def export_metrics(
        self,
        metrics: Dict[str, Any],
        filename: Optional[str] = None
    ) -> str:
        """
        导出性能指标
        
        Args:
            metrics: 性能指标字典
            filename: 自定义文件名（可选）
            
        Returns:
            导出的文件路径
        """
        if not metrics:
            raise ValueError("导出数据不能为空")
        
        # 准备数据
        data = [{
            "timestamp": datetime.now().isoformat(),
            **metrics
        }]
        
        # 创建 DataFrame
        df = pd.DataFrame(data)
        
        # 确定输出文件路径
        output_path = filename if filename else self._generate_filename("metrics")
        
        # 导出到 CSV
        df.to_csv(output_path, index=False)
        return output_path
    
    def export_batch_results(
        self,
        batch_results: List[Dict[str, Any]],
        filename: Optional[str] = None
    ) -> str:
        """
        导出批量处理结果
        
        Args:
            batch_results: 批量处理结果列表
            filename: 自定义文件名（可选）
            
        Returns:
            导出的文件路径
        """
        if not batch_results:
            raise ValueError("导出数据不能为空")
        
        # 准备数据
        data = []
        for batch in batch_results:
            row = {
                "timestamp": datetime.now().isoformat(),
                "batch_size": len(batch.get("results", [])),
                "total_time": batch.get("total_time"),
                "average_time": batch.get("average_time")
            }
            
            # 添加额外的元数据
            if "metadata" in batch:
                row.update(batch["metadata"])
            
            data.append(row)
        
        # 创建 DataFrame
        df = pd.DataFrame(data)
        
        # 确定输出文件路径
        output_path = filename if filename else self._generate_filename("batch")
        
        # 导出到 CSV
        df.to_csv(output_path, index=False)
        return output_path
</file>

<file path="app/metrics/collector.py">
import time
import psutil
import torch
from typing import Dict, List, Optional
from dataclasses import dataclass
from datetime import datetime


@dataclass
class InferenceMetrics:
    """推理指标数据类"""
    inference_time: float  # 推理时间（秒）
    memory_usage: float    # 内存使用（MB）
    gpu_memory: Optional[float] = None  # GPU 内存使用（MB）
    batch_size: int = 1    # 批次大小
    image_size: Optional[tuple] = None  # 图像尺寸


class MetricsCollector:
    """性能指标收集器"""
    
    def __init__(self):
        """初始化指标收集器"""
        self._metrics_history: List[InferenceMetrics] = []
        self._start_time: Optional[float] = None
        self._process = psutil.Process()
    
    def start_timer(self) -> None:
        """开始计时"""
        self._start_time = time.time()
    
    def stop_timer(self) -> float:
        """
        停止计时并返回持续时间
        
        Returns:
            持续时间（秒）
        """
        if self._start_time is None:
            return 0.0
        duration = time.time() - self._start_time
        self._start_time = None
        return duration
    
    def get_memory_usage(self) -> float:
        """
        获取内存使用情况
        
        Returns:
            内存使用量（MB）
        """
        return self._process.memory_info().rss / 1024 / 1024
    
    def get_gpu_memory_usage(self) -> Optional[float]:
        """
        获取 GPU 内存使用情况
        
        Returns:
            GPU 内存使用量（MB），如果没有 GPU 则返回 None
        """
        if not torch.cuda.is_available():
            return None
        return torch.cuda.memory_allocated() / 1024 / 1024
    
    def collect_metrics(self, batch_size: int = 1, image_size: Optional[tuple] = None) -> Dict[str, any]:
        """
        收集当前推理的指标
        
        Args:
            batch_size: 批次大小
            image_size: 图像尺寸
            
        Returns:
            包含推理指标的字典
        """
        inference_time = self.stop_timer()
        memory_usage = self.get_memory_usage()
        gpu_memory = self.get_gpu_memory_usage()
        
        # 创建一个普通字典而不是使用InferenceMetrics类
        metrics = {
            "inference_time": float(inference_time),
            "memory_usage": float(memory_usage),
            "gpu_memory": float(gpu_memory) if gpu_memory is not None else None,
            "batch_size": int(batch_size)
        }
        
        # 将图像尺寸转换为可序列化格式
        if image_size is not None:
            metrics["image_width"] = int(image_size[0])
            metrics["image_height"] = int(image_size[1])
        
        # 保存指标历史记录 (使用InferenceMetrics对象仅用于内部存储)
        self._metrics_history.append(InferenceMetrics(
            inference_time=inference_time,
            memory_usage=memory_usage,
            gpu_memory=gpu_memory,
            batch_size=batch_size,
            image_size=image_size
        ))
        
        return metrics
    
    def get_average_metrics(self) -> Dict[str, float]:
        """
        获取平均指标
        
        Returns:
            包含平均指标的字典
        """
        if not self._metrics_history:
            return {}
        
        total_time = sum(m.inference_time for m in self._metrics_history)
        total_memory = sum(m.memory_usage for m in self._metrics_history)
        total_gpu_memory = sum(m.gpu_memory for m in self._metrics_history if m.gpu_memory is not None)
        
        count = len(self._metrics_history)
        gpu_count = sum(1 for m in self._metrics_history if m.gpu_memory is not None)
        
        metrics = {
            "average_inference_time": total_time / count,
            "average_memory_usage": total_memory / count,
            "total_inferences": count
        }
        
        if gpu_count > 0:
            metrics["average_gpu_memory"] = total_gpu_memory / gpu_count
        
        return metrics
    
    def export_metrics(self, file_path: str) -> None:
        """
        导出指标到文件
        
        Args:
            file_path: 导出文件路径
        """
        import pandas as pd
        
        data = []
        for metrics in self._metrics_history:
            row = {
                "timestamp": datetime.now().isoformat(),
                "inference_time": metrics.inference_time,
                "memory_usage": metrics.memory_usage,
                "batch_size": metrics.batch_size
            }
            if metrics.gpu_memory is not None:
                row["gpu_memory"] = metrics.gpu_memory
            if metrics.image_size is not None:
                row["image_width"] = metrics.image_size[0]
                row["image_height"] = metrics.image_size[1]
            data.append(row)
        
        df = pd.DataFrame(data)
        df.to_csv(file_path, index=False)
    
    def clear_history(self) -> None:
        """清空历史记录"""
        self._metrics_history.clear()
</file>

<file path="app/models/__init__.py">
from .base import ModelFactory
from .mobilenet import MobileNetModel

# 注册 MobileNet 模型
ModelFactory.register_model("mobilenet", MobileNetModel)
</file>

<file path="app/models/base.py">
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional, Tuple
import torch
import numpy as np
from pathlib import Path


class BaseModel(ABC):
    """基础模型接口类"""
    
    def __init__(self, model_path: Optional[str] = None, device: str = "cpu"):
        """
        初始化模型
        
        Args:
            model_path: 模型文件路径
            device: 运行设备，可选 "cpu" 或 "cuda"
        """
        self.device = device
        self.model = None
        if model_path:
            self.load_model(model_path)
    
    @abstractmethod
    def load_model(self, model_path: str) -> None:
        """
        加载模型
        
        Args:
            model_path: 模型文件路径
        """
        pass
    
    @abstractmethod
    def preprocess(self, image: np.ndarray) -> torch.Tensor:
        """
        图像预处理
        
        Args:
            image: 输入图像，numpy 数组格式
            
        Returns:
            预处理后的张量
        """
        pass
    
    @abstractmethod
    def postprocess(self, output: torch.Tensor) -> List[Dict[str, Any]]:
        """
        后处理推理结果
        
        Args:
            output: 模型输出张量
            
        Returns:
            处理后的结果列表，每个结果包含类别、置信度等信息
        """
        pass
    
    def predict(self, image: np.ndarray) -> List[Dict[str, Any]]:
        """
        执行推理
        
        Args:
            image: 输入图像，numpy 数组格式
            
        Returns:
            推理结果列表
        """
        # 预处理
        input_tensor = self.preprocess(image)
        input_tensor = input_tensor.to(self.device)
        
        # 推理
        with torch.no_grad():
            output = self.model(input_tensor)
        
        # 后处理
        results = self.postprocess(output)
        return results
    
    def to(self, device: str) -> None:
        """
        切换运行设备
        
        Args:
            device: 目标设备，"cpu" 或 "cuda"
        """
        if self.model:
            self.model = self.model.to(device)
        self.device = device


class ModelFactory:
    """模型工厂类"""
    
    _models = {}
    
    @classmethod
    def register_model(cls, name: str, model_class: type) -> None:
        """
        注册模型类
        
        Args:
            name: 模型名称
            model_class: 模型类
        """
        cls._models[name] = model_class
    
    @classmethod
    def create_model(cls, name: str, **kwargs) -> BaseModel:
        """
        创建模型实例
        
        Args:
            name: 模型名称
            **kwargs: 模型初始化参数
            
        Returns:
            模型实例
            
        Raises:
            ValueError: 如果模型名称未注册
        """
        if name not in cls._models:
            raise ValueError(f"Model {name} not registered")
        return cls._models[name](**kwargs)
    
    @classmethod
    def get_available_models(cls) -> List[str]:
        """
        获取所有可用的模型名称
        
        Returns:
            模型名称列表
        """
        return list(cls._models.keys())
</file>

<file path="app/models/mobilenet.py">
import torch
import torchvision
import numpy as np
from typing import Any, Dict, List
from .base import BaseModel


class MobileNetModel(BaseModel):
    """MobileNet 模型实现"""
    
    def __init__(self, model_path: str = None, device: str = "cpu"):
        """
        初始化 MobileNet 模型
        
        Args:
            model_path: 模型文件路径
            device: 运行设备
        """
        super().__init__(model_path, device)
        self.class_names = [
            "限速20", "限速30", "限速50", "限速60", "限速70", "限速80", "限速100", "限速120",
            "禁止通行", "禁止左转", "禁止右转", "禁止直行", "禁止掉头",
            "注意行人", "注意儿童", "注意非机动车", "注意野生动物",
            "前方施工", "前方拥堵", "前方事故",
            "停车让行", "减速让行",
            "直行", "左转", "右转", "掉头",
            "人行横道", "非机动车道",
            "公交专用", "应急车道",
            "其他"
        ]
    
    def load_model(self, model_path: str) -> None:
        """
        加载模型
        
        Args:
            model_path: 模型文件路径
        """
        # 加载预训练的 MobileNet 模型
        self.model = torchvision.models.mobilenet_v2(weights=torchvision.models.MobileNet_V2_Weights.IMAGENET1K_V1)
        
        # 修改最后一层以适应我们的分类任务
        num_classes = len(self.class_names)
        self.model.classifier[1] = torch.nn.Linear(
            self.model.classifier[1].in_features,
            num_classes
        )
        
        # 如果有预训练权重，则加载
        if model_path:
            state_dict = torch.load(model_path, map_location=self.device)
            self.model.load_state_dict(state_dict)
        
        self.model = self.model.to(self.device)
        self.model.eval()
    
    def preprocess(self, image: np.ndarray) -> torch.Tensor:
        """
        图像预处理
        
        Args:
            image: 输入图像，numpy 数组格式
            
        Returns:
            预处理后的张量
        """
        # 检查图像是否为None
        if image is None:
            raise ValueError("输入图像不能为None")
            
        # 确保图像是numpy数组
        if not isinstance(image, np.ndarray):
            raise TypeError("输入图像必须是numpy数组")
        
        # 检查图像数据
        if image.size == 0 or len(image.shape) < 2:
            raise ValueError("输入图像数据异常，无法处理")
        
        # 转换为 RGB 格式
        if len(image.shape) == 2:
            image = np.stack([image] * 3, axis=-1)
        elif len(image.shape) > 2 and image.shape[2] == 4:
            image = image[:, :, :3]
        elif len(image.shape) < 3:
            raise ValueError("输入图像格式错误，无法转换为RGB格式")
        
        try:
            # 调整大小
            image = torchvision.transforms.functional.resize(
                torch.from_numpy(image).permute(2, 0, 1),
                (224, 224)
            )
            
            # 标准化
            image = image.float() / 255.0
            image = torchvision.transforms.functional.normalize(
                image,
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            )
            
            # 添加批次维度
            image = image.unsqueeze(0)
            return image
        except Exception as e:
            raise ValueError(f"图像预处理失败: {str(e)}")
    
    def postprocess(self, output: torch.Tensor) -> List[Dict[str, Any]]:
        """
        后处理推理结果
        
        Args:
            output: 模型输出张量
            
        Returns:
            处理后的结果列表
        """
        # 检查输出是否为None
        if output is None:
            return []
            
        # 检查输出是否为张量
        if not isinstance(output, torch.Tensor):
            raise TypeError("输出必须是PyTorch张量")
            
        try:
            # 获取预测结果
            probabilities = torch.nn.functional.softmax(output, dim=1)
            top_prob, top_class = torch.topk(probabilities, 1)
            
            # 转换为列表格式
            results = []
            for i in range(len(top_class)):
                class_id = int(top_class[i].item())
                # 确保类别ID在有效范围内
                if 0 <= class_id < len(self.class_names):
                    result = {
                        "class_id": class_id,
                        "class_name": self.class_names[class_id],
                        "confidence": float(top_prob[i].item())
                    }
                else:
                    # 如果类别ID超出范围，使用默认值
                    result = {
                        "class_id": class_id,
                        "class_name": "未知类别",
                        "confidence": float(top_prob[i].item())
                    }
                results.append(result)
            
            return results
        except Exception as e:
            # 异常情况下返回空列表
            print(f"后处理错误: {str(e)}")
            return []
</file>

<file path="app/processors/image.py">
import cv2
import numpy as np
from typing import List, Optional, Tuple, Union
from pathlib import Path


class ImageProcessor:
    """图像处理器类"""
    
    def __init__(self):
        """初始化图像处理器"""
        self._supported_formats = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}
    
    def load_image(self, image_path: Union[str, Path]) -> Optional[np.ndarray]:
        """
        加载图像
        
        Args:
            image_path: 图像文件路径
            
        Returns:
            加载的图像数组，如果加载失败则返回 None
        """
        try:
            image = cv2.imread(str(image_path))
            if image is None:
                return None
            # 转换为 RGB 格式
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            return image
        except Exception as e:
            print(f"Error loading image {image_path}: {str(e)}")
            return None
    
    def preprocess(self, image: np.ndarray, target_size: Tuple[int, int] = (224, 224)) -> np.ndarray:
        """
        图像预处理
        
        Args:
            image: 输入图像
            target_size: 目标尺寸
            
        Returns:
            预处理后的图像
        """
        # 检查图像是否为None
        if image is None:
            raise ValueError("输入图像不能为None")
        
        # 确保图像是numpy数组并且有正确的维度
        if not isinstance(image, np.ndarray):
            raise TypeError("输入图像必须是numpy数组")
        
        # 检查图像是否为空或尺寸异常
        if image.size == 0 or len(image.shape) < 2:
            raise ValueError("输入图像数据异常，无法处理")
        
        # 调整大小
        try:
            image = cv2.resize(image, target_size)
        except Exception as e:
            raise ValueError(f"图像大小调整失败: {str(e)}")
        
        # 归一化
        image = image.astype(np.float32) / 255.0
        
        return image
    
    def batch_preprocess(self, images: List[np.ndarray], target_size: Tuple[int, int] = (224, 224)) -> List[np.ndarray]:
        """
        批量图像预处理
        
        Args:
            images: 输入图像列表
            target_size: 目标尺寸
            
        Returns:
            预处理后的图像列表
        """
        return [self.preprocess(img, target_size) for img in images]
    
    def is_supported_format(self, file_path: Union[str, Path]) -> bool:
        """
        检查文件格式是否支持
        
        Args:
            file_path: 文件路径
            
        Returns:
            是否支持该格式
        """
        return Path(file_path).suffix.lower() in self._supported_formats
    
    def draw_detection(self, image: np.ndarray, results: List[dict], 
                      font_scale: float = 0.6, thickness: int = 2) -> np.ndarray:
        """
        在图像上绘制检测结果
        
        Args:
            image: 原始图像
            results: 检测结果列表
            font_scale: 字体大小
            thickness: 线条粗细
            
        Returns:
            绘制了检测结果的图像
        """
        # 转换为 BGR 格式用于显示
        display_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
        
        for result in results:
            class_name = result.get('class_name', 'Unknown')
            confidence = result.get('confidence', 0.0)
            
            # 在图像上添加文本
            text = f"{class_name}: {confidence:.2f}"
            cv2.putText(display_image, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,
                       font_scale, (0, 255, 0), thickness)
        
        return display_image
</file>

<file path="app/main.py">
import os
import gradio as gr
import numpy as np
from typing import Dict, List, Optional
from PIL import Image

from app.models import ModelFactory
from app.processors.image import ImageProcessor
from app.metrics.collector import MetricsCollector
from app.exporters.csv import CSVExporter


class TrafficSignRecognitionApp:
    """交通标识识别应用"""
    
    def __init__(self):
        """初始化应用"""
        # 初始化组件
        self.model_factory = ModelFactory()
        self.image_processor = ImageProcessor()
        self.metrics_collector = MetricsCollector()
        self.csv_exporter = CSVExporter()
        
        # 获取可用模型列表
        self.available_models = ModelFactory.get_available_models()
        
        # 初始化当前模型
        self.current_model = None
        self.current_model_name = None
        if self.available_models:
            self.load_model(self.available_models[0])
    
    def load_model(self, model_name: str) -> None:
        """
        加载模型
        
        Args:
            model_name: 模型名称
        """
        if model_name != self.current_model_name:
            self.current_model = self.model_factory.create_model(model_name)
            self.current_model_name = model_name
    
    def process_image(
        self,
        image: np.ndarray,
        model_name: str
    ) -> Dict[str, any]:
        """
        处理单张图像
        
        Args:
            image: 输入图像
            model_name: 模型名称
            
        Returns:
            处理结果字典
        """
        # 检查图像是否为None
        if image is None:
            return {
                "error": "输入图像为空",
                "image": None,
                "predictions": [],
                "metrics": {}
            }
            
        # 加载模型
        self.load_model(model_name)
        
        # 开始收集指标
        self.metrics_collector.start_timer()
        
        try:
            # 预处理图像
            processed_image = self.image_processor.preprocess(image)
            
            # 模型推理
            predictions = self.current_model.predict(processed_image)
            
            # 收集指标
            metrics = self.metrics_collector.collect_metrics(
                batch_size=1,
                image_size=image.shape[:2]
            )
            
            # 准备结果
            result = {
                "predictions": predictions,
                "metrics": metrics,
                "image": image
            }
            
            return result
            
        except Exception as e:
            # 停止计时，确保不会影响下一次请求
            self.metrics_collector.stop_timer()
            
            # 返回标准化的错误信息
            return {
                "error": str(e),
                "image": image,
                "predictions": [],
                "metrics": {}
            }
    
    def process_batch(
        self,
        images: List[np.ndarray],
        model_name: str
    ) -> Dict[str, any]:
        """
        批量处理图像
        
        Args:
            images: 输入图像列表
            model_name: 模型名称
            
        Returns:
            处理结果字典
        """
        # 检查输入图像列表是否为空
        if not images:
            return {
                "error": "输入图像列表为空",
                "results": [],
                "total_time": 0.0,
                "average_time": 0.0
            }
            
        # 加载模型
        self.load_model(model_name)
        
        # 开始收集指标
        self.metrics_collector.start_timer()
        
        try:
            results = []
            total_time = 0.0
            
            for image in images:
                # 检查单个图像是否为None
                if image is None:
                    results.append({
                        "error": "输入图像为空",
                        "predictions": [],
                        "metrics": {},
                        "image": None
                    })
                    continue
                
                # 预处理图像
                processed_image = self.image_processor.preprocess(image)
                
                # 模型推理
                predictions = self.current_model.predict(processed_image)
                
                # 收集指标
                metrics = self.metrics_collector.collect_metrics(
                    batch_size=1,
                    image_size=image.shape[:2]
                )
                
                results.append({
                    "predictions": predictions,
                    "metrics": metrics,
                    "image": image
                })
                
                total_time += metrics.get("inference_time", 0.0)
            
            # 计算平均时间，避免除以零的情况
            average_time = total_time / len(images) if len(images) > 0 else 0.0
            
            return {
                "results": results,
                "total_time": float(total_time),
                "average_time": float(average_time)
            }
            
        except Exception as e:
            # 停止计时，确保不会影响下一次请求
            self.metrics_collector.stop_timer()
            
            # 返回标准化的错误信息
            return {
                "error": str(e),
                "results": [],
                "total_time": 0.0,
                "average_time": 0.0
            }
    
    def export_results(
        self,
        results: Dict[str, any],
        export_type: str
    ) -> str:
        """
        导出结果
        
        Args:
            results: 处理结果
            export_type: 导出类型
            
        Returns:
            导出文件路径
        """
        try:
            if export_type == "inference":
                return self.csv_exporter.export_inference_results(
                    results["predictions"]
                )
            elif export_type == "metrics":
                return self.csv_exporter.export_metrics(
                    results["metrics"]
                )
            elif export_type == "batch":
                return self.csv_exporter.export_batch_results(
                    [results]
                )
            else:
                raise ValueError(f"不支持的导出类型: {export_type}")
                
        except Exception as e:
            return f"导出失败: {str(e)}"


def create_interface():
    """创建 Gradio 界面"""
    app = TrafficSignRecognitionApp()
    
    with gr.Blocks(title="交通标识识别系统") as interface:
        gr.Markdown("# 交通标识识别系统")
        
        with gr.Row():
            with gr.Column():
                # 模型选择
                model_dropdown = gr.Dropdown(
                    choices=app.available_models,
                    label="选择模型",
                    value=app.available_models[0] if app.available_models else None
                )
                
                # 图像输入
                image_input = gr.Image(
                    label="输入图像",
                    type="numpy"
                )
                
                # 处理按钮
                process_btn = gr.Button("处理图像")
                
                # 批量处理
                batch_input = gr.File(
                    label="批量处理",
                    file_count="multiple",
                    file_types=["image"]
                )
                
                batch_process_btn = gr.Button("批量处理")
            
            with gr.Column():
                # 结果显示
                result_output = gr.JSON(
                    label="处理结果"
                )
                
                # 性能指标
                metrics_output = gr.JSON(
                    label="性能指标"
                )
                
                # 导出选项
                export_type = gr.Radio(
                    choices=["inference", "metrics", "batch"],
                    label="导出类型",
                    value="inference"
                )
                
                export_btn = gr.Button("导出结果")
                
                # 导出结果
                export_output = gr.Text(
                    label="导出结果"
                )
        
        # 单张图像处理
        process_btn.click(
            fn=app.process_image,
            inputs=[image_input, model_dropdown],
            outputs=[result_output, metrics_output]
        )
        
        # 批量处理
        batch_process_btn.click(
            fn=app.process_batch,
            inputs=[batch_input, model_dropdown],
            outputs=[result_output, metrics_output]
        )
        
        # 导出结果
        export_btn.click(
            fn=app.export_results,
            inputs=[result_output, export_type],
            outputs=export_output
        )
    
    return interface


if __name__ == "__main__":
    interface = create_interface()
    interface.launch(share=True)
</file>

<file path="tests/test_app.py">
import unittest
import numpy as np
from app.main import TrafficSignRecognitionApp


class TestTrafficSignRecognitionApp(unittest.TestCase):
    """交通标识识别应用测试类"""
    
    def setUp(self):
        """测试准备"""
        self.app = TrafficSignRecognitionApp()
        self.test_image = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)
    
    def test_initialization(self):
        """测试初始化"""
        # 检查组件初始化
        self.assertIsNotNone(self.app.model_factory)
        self.assertIsNotNone(self.app.image_processor)
        self.assertIsNotNone(self.app.metrics_collector)
        self.assertIsNotNone(self.app.csv_exporter)
        
        # 检查模型列表
        self.assertIsInstance(self.app.available_models, list)
        self.assertIn("mobilenet", self.app.available_models)
    
    def test_load_model(self):
        """测试模型加载"""
        # 测试加载 MobileNet
        self.app.load_model("mobilenet")
        self.assertIsNotNone(self.app.current_model)
        self.assertEqual(self.app.current_model_name, "mobilenet")
        
        # 测试重复加载
        original_model = self.app.current_model
        self.app.load_model("mobilenet")
        self.assertEqual(self.app.current_model, original_model)
    
    def test_process_image(self):
        """测试图像处理"""
        # 加载模型
        self.app.load_model("mobilenet")
        
        # 处理图像
        result = self.app.process_image(self.test_image, "mobilenet")
        
        # 检查结果
        self.assertIsInstance(result, dict)
        self.assertIn("predictions", result)
        self.assertIn("metrics", result)
        self.assertIn("image", result)
        
        # 检查预测结果
        predictions = result["predictions"]
        self.assertIsInstance(predictions, list)
        self.assertTrue(all(isinstance(p, dict) for p in predictions))
        
        # 检查性能指标
        metrics = result["metrics"]
        self.assertIsInstance(metrics, dict)
        self.assertIn("inference_time", metrics)
        self.assertIn("memory_usage", metrics)
    
    def test_process_batch(self):
        """测试批量处理"""
        # 准备批量图像
        batch_images = [self.test_image] * 3
        
        # 处理批量图像
        result = self.app.process_batch(batch_images, "mobilenet")
        
        # 检查结果
        self.assertIsInstance(result, dict)
        self.assertIn("results", result)
        self.assertIn("total_time", result)
        self.assertIn("average_time", result)
        
        # 检查批量结果
        results = result["results"]
        self.assertEqual(len(results), 3)
        self.assertTrue(all(isinstance(r, dict) for r in results))
    
    def test_export_results(self):
        """测试结果导出"""
        # 处理图像
        result = self.app.process_image(self.test_image, "mobilenet")
        
        # 测试导出推理结果
        export_path = self.app.export_results(result, "inference")
        self.assertTrue(export_path.endswith(".csv"))
        
        # 测试导出性能指标
        export_path = self.app.export_results(result, "metrics")
        self.assertTrue(export_path.endswith(".csv"))
        
        # 测试导出批量结果
        batch_result = self.app.process_batch([self.test_image], "mobilenet")
        export_path = self.app.export_results(batch_result, "batch")
        self.assertTrue(export_path.endswith(".csv"))
        
        # 测试无效导出类型
        with self.assertRaises(ValueError):
            self.app.export_results(result, "invalid")


if __name__ == "__main__":
    unittest.main()
</file>

<file path="tests/test_exporters.py">
import unittest
import os
import pandas as pd
from app.exporters.csv import CSVExporter


class TestCSVExporter(unittest.TestCase):
    """CSV 导出器测试类"""
    
    def setUp(self):
        """测试准备"""
        self.exporter = CSVExporter("test_exports")
        self.test_results = [{
            "class_id": 1,
            "class_name": "stop",
            "confidence": 0.95,
            "metadata": {
                "image_size": (224, 224),
                "processing_time": 0.1
            }
        }]
        
        self.test_metrics = {
            "inference_time": 0.1,
            "memory_usage": 100.0,
            "gpu_memory": 200.0
        }
        
        self.test_batch = [{
            "results": self.test_results,
            "total_time": 0.3,
            "average_time": 0.1
        }]
    
    def tearDown(self):
        """测试清理"""
        # 清理测试文件
        if os.path.exists("test_exports"):
            for file in os.listdir("test_exports"):
                os.remove(os.path.join("test_exports", file))
            os.rmdir("test_exports")
    
    def test_export_inference_results(self):
        """测试推理结果导出"""
        # 导出结果
        file_path = self.exporter.export_inference_results(self.test_results)
        
        # 检查导出文件
        self.assertTrue(os.path.exists(file_path))
        df = pd.read_csv(file_path)
        
        # 检查数据
        self.assertEqual(len(df), 1)
        self.assertIn("class_id", df.columns)
        self.assertIn("class_name", df.columns)
        self.assertIn("confidence", df.columns)
    
    def test_export_metrics(self):
        """测试性能指标导出"""
        # 导出指标
        file_path = self.exporter.export_metrics(self.test_metrics)
        
        # 检查导出文件
        self.assertTrue(os.path.exists(file_path))
        df = pd.read_csv(file_path)
        
        # 检查数据
        self.assertEqual(len(df), 1)
        self.assertIn("inference_time", df.columns)
        self.assertIn("memory_usage", df.columns)
        self.assertIn("gpu_memory", df.columns)
    
    def test_export_batch_results(self):
        """测试批量结果导出"""
        # 导出结果
        file_path = self.exporter.export_batch_results(self.test_batch)
        
        # 检查导出文件
        self.assertTrue(os.path.exists(file_path))
        df = pd.read_csv(file_path)
        
        # 检查数据
        self.assertEqual(len(df), 1)
        self.assertIn("batch_size", df.columns)
        self.assertIn("total_time", df.columns)
        self.assertIn("average_time", df.columns)
    
    def test_custom_filename(self):
        """测试自定义文件名"""
        # 测试推理结果导出
        custom_path = "custom_inference.csv"
        file_path = self.exporter.export_inference_results(
            self.test_results,
            custom_path
        )
        self.assertEqual(file_path, custom_path)
        
        # 测试性能指标导出
        custom_path = "custom_metrics.csv"
        file_path = self.exporter.export_metrics(
            self.test_metrics,
            custom_path
        )
        self.assertEqual(file_path, custom_path)
        
        # 测试批量结果导出
        custom_path = "custom_batch.csv"
        file_path = self.exporter.export_batch_results(
            self.test_batch,
            custom_path
        )
        self.assertEqual(file_path, custom_path)
    
    def test_empty_data(self):
        """测试空数据导出"""
        # 测试空推理结果
        with self.assertRaises(ValueError):
            self.exporter.export_inference_results([])
        
        # 测试空性能指标
        with self.assertRaises(ValueError):
            self.exporter.export_metrics({})
        
        # 测试空批量结果
        with self.assertRaises(ValueError):
            self.exporter.export_batch_results([])


if __name__ == "__main__":
    unittest.main()
</file>

<file path="tests/test_metrics.py">
import unittest
import time
from app.metrics.collector import MetricsCollector, InferenceMetrics


class TestMetricsCollector(unittest.TestCase):
    """性能指标收集器测试类"""
    
    def setUp(self):
        """测试准备"""
        self.collector = MetricsCollector()
    
    def test_timer(self):
        """测试计时功能"""
        # 测试开始计时
        self.collector.start_timer()
        self.assertIsNotNone(self.collector._start_time)
        
        # 测试停止计时
        time.sleep(0.1)  # 等待一段时间
        duration = self.collector.stop_timer()
        self.assertIsNone(self.collector._start_time)
        self.assertGreater(duration, 0)
    
    def test_memory_usage(self):
        """测试内存使用统计"""
        # 测试获取内存使用
        memory = self.collector.get_memory_usage()
        self.assertIsInstance(memory, float)
        self.assertGreater(memory, 0)
    
    def test_collect_metrics(self):
        """测试指标收集"""
        # 开始计时
        self.collector.start_timer()
        time.sleep(0.1)  # 等待一段时间
        
        # 收集指标
        metrics = self.collector.collect_metrics(
            batch_size=2,
            image_size=(224, 224)
        )
        
        # 检查指标
        self.assertIsInstance(metrics, InferenceMetrics)
        self.assertGreater(metrics.inference_time, 0)
        self.assertGreater(metrics.memory_usage, 0)
        self.assertEqual(metrics.batch_size, 2)
        self.assertEqual(metrics.image_size, (224, 224))
    
    def test_get_average_metrics(self):
        """测试平均指标计算"""
        # 收集多个指标
        for _ in range(3):
            self.collector.start_timer()
            time.sleep(0.1)
            self.collector.collect_metrics()
        
        # 获取平均指标
        averages = self.collector.get_average_metrics()
        
        # 检查平均指标
        self.assertIsInstance(averages, dict)
        self.assertIn("average_inference_time", averages)
        self.assertIn("average_memory_usage", averages)
        self.assertIn("total_inferences", averages)
        self.assertEqual(averages["total_inferences"], 3)
    
    def test_export_metrics(self):
        """测试指标导出"""
        import os
        import pandas as pd
        
        # 收集一些指标
        self.collector.start_timer()
        time.sleep(0.1)
        self.collector.collect_metrics()
        
        # 导出指标
        file_path = "test_metrics.csv"
        self.collector.export_metrics(file_path)
        
        # 检查导出文件
        self.assertTrue(os.path.exists(file_path))
        df = pd.read_csv(file_path)
        self.assertFalse(df.empty)
        
        # 清理测试文件
        os.remove(file_path)
    
    def test_clear_history(self):
        """测试历史记录清理"""
        # 收集一些指标
        self.collector.start_timer()
        time.sleep(0.1)
        self.collector.collect_metrics()
        
        # 清理历史记录
        self.collector.clear_history()
        
        # 检查历史记录
        self.assertEqual(len(self.collector._metrics_history), 0)


if __name__ == "__main__":
    unittest.main()
</file>

<file path="tests/test_models.py">
import unittest
import numpy as np
import torch
from app.models import ModelFactory
from app.models.mobilenet import MobileNetModel


class TestMobileNetModel(unittest.TestCase):
    """MobileNet 模型测试类"""
    
    def setUp(self):
        """测试准备"""
        self.model = MobileNetModel()
        self.test_image = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)
    
    def test_load_model(self):
        """测试模型加载"""
        # 测试默认加载
        self.model.load_model()
        self.assertIsNotNone(self.model.model)
        
        # 测试自定义路径加载
        self.model.load_model("path/to/model.pth")
        self.assertIsNotNone(self.model.model)
    
    def test_preprocess(self):
        """测试图像预处理"""
        # 测试单张图像
        processed = self.model.preprocess(self.test_image)
        self.assertIsInstance(processed, torch.Tensor)
        self.assertEqual(processed.shape, (1, 3, 224, 224))
        
        # 测试批量图像
        batch_images = [self.test_image] * 2
        processed = self.model.preprocess(batch_images)
        self.assertIsInstance(processed, torch.Tensor)
        self.assertEqual(processed.shape, (2, 3, 224, 224))
    
    def test_postprocess(self):
        """测试后处理"""
        # 创建模拟输出
        mock_output = torch.randn(1, len(self.model.class_names))
        
        # 测试后处理
        results = self.model.postprocess(mock_output)
        self.assertIsInstance(results, list)
        self.assertTrue(all(isinstance(r, dict) for r in results))
        self.assertTrue(all("class_id" in r for r in results))
        self.assertTrue(all("class_name" in r for r in results))
        self.assertTrue(all("confidence" in r for r in results))


class TestModelFactory(unittest.TestCase):
    """模型工厂测试类"""
    
    def setUp(self):
        """测试准备"""
        self.factory = ModelFactory()
    
    def test_get_available_models(self):
        """测试获取可用模型列表"""
        models = self.factory.get_available_models()
        self.assertIsInstance(models, list)
        self.assertIn("mobilenet", models)
    
    def test_create_model(self):
        """测试创建模型"""
        # 测试创建 MobileNet
        model = self.factory.create_model("mobilenet")
        self.assertIsInstance(model, MobileNetModel)
        
        # 测试创建不存在的模型
        with self.assertRaises(ValueError):
            self.factory.create_model("nonexistent")


if __name__ == "__main__":
    unittest.main()
</file>

<file path="tests/test_processors.py">
import unittest
import numpy as np
from app.processors.image import ImageProcessor


class TestImageProcessor(unittest.TestCase):
    """图像处理器测试类"""
    
    def setUp(self):
        """测试准备"""
        self.processor = ImageProcessor()
        self.test_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
    
    def test_is_supported_format(self):
        """测试格式检查"""
        # 测试支持的格式
        self.assertTrue(self.processor.is_supported_format("test.jpg"))
        self.assertTrue(self.processor.is_supported_format("test.png"))
        self.assertTrue(self.processor.is_supported_format("test.jpeg"))
        
        # 测试不支持的格式
        self.assertFalse(self.processor.is_supported_format("test.txt"))
        self.assertFalse(self.processor.is_supported_format("test.pdf"))
    
    def test_preprocess(self):
        """测试图像预处理"""
        # 测试默认尺寸
        processed = self.processor.preprocess(self.test_image)
        self.assertIsInstance(processed, np.ndarray)
        self.assertEqual(processed.shape, (224, 224, 3))
        
        # 测试自定义尺寸
        processed = self.processor.preprocess(self.test_image, target_size=(128, 128))
        self.assertIsInstance(processed, np.ndarray)
        self.assertEqual(processed.shape, (128, 128, 3))
        
        # 测试数值范围
        self.assertTrue(np.all(processed >= 0))
        self.assertTrue(np.all(processed <= 1))
    
    def test_batch_preprocess(self):
        """测试批量预处理"""
        # 准备批量图像
        batch_images = [self.test_image] * 3
        
        # 测试批量处理
        processed = self.processor.batch_preprocess(batch_images)
        self.assertIsInstance(processed, list)
        self.assertEqual(len(processed), 3)
        
        # 检查每张图像
        for img in processed:
            self.assertIsInstance(img, np.ndarray)
            self.assertEqual(img.shape, (224, 224, 3))
    
    def test_draw_detection(self):
        """测试检测结果绘制"""
        # 准备检测结果
        detections = [{
            "class_name": "stop",
            "confidence": 0.95,
            "bbox": [100, 100, 200, 200]
        }]
        
        # 测试绘制
        result = self.processor.draw_detection(self.test_image, detections)
        self.assertIsInstance(result, np.ndarray)
        self.assertEqual(result.shape, self.test_image.shape)


if __name__ == "__main__":
    unittest.main()
</file>

<file path=".cursorrules">
# RIPER-5 + MULTIDIMENSIONAL THINKING + AGENT EXECUTION PROTOCOL



## Table of Contents
- [RIPER-5 + MULTIDIMENSIONAL THINKING + AGENT EXECUTION PROTOCOL](#riper-5--multidimensional-thinking--agent-execution-protocol)
  - [Table of Contents](#table-of-contents)
  - [Context and Settings](#context-and-settings)
  - [Core Thinking Principles](#core-thinking-principles)
  - [Mode Details](#mode-details)
    - [Mode 1: RESEARCH](#mode-1-research)
    - [Mode 2: INNOVATE](#mode-2-innovate)
    - [Mode 3: PLAN](#mode-3-plan)
    - [Mode 4: EXECUTE](#mode-4-execute)
    - [Mode 5: REVIEW](#mode-5-review)
  - [Key Protocol Guidelines](#key-protocol-guidelines)
  - [Code Handling Guidelines](#code-handling-guidelines)
  - [Task File Template](#task-file-template)
  - [Performance Expectations](#performance-expectations)

## Context and Settings
<a id="context-and-settings"></a>

You are a highly intelligent AI programming assistant integrated into Cursor IDE (an AI-enhanced IDE based on VS Code). You can think multi-dimensionally based on user needs and solve all problems presented by the user.

> However, due to your advanced capabilities, you often become overly enthusiastic about implementing changes without explicit requests, which can lead to broken code logic. To prevent this, you must strictly follow this protocol.

**Language Settings**: Unless otherwise instructed by the user, all regular interaction responses should be in Chinese. However, mode declarations (e.g., [MODE: RESEARCH]) and specific formatted outputs (e.g., code blocks) should remain in English to ensure format consistency.

**Automatic Mode Initiation**: This optimized version supports automatic initiation of all modes without explicit transition commands. Each mode will automatically proceed to the next upon completion.

**Mode Declaration Requirement**: You must declare the current mode in square brackets at the beginning of every response, without exception. Format: `[MODE: MODE_NAME]`

**Initial Default Mode**:
*   Default starts in **RESEARCH** mode.
*   **Exceptions**: If the user's initial request clearly points to a specific phase, you can directly enter the corresponding mode.
    *   *Example 1*: User provides a detailed step plan and says "Execute this plan" -> Can directly enter PLAN mode (for plan validation first) or EXECUTE mode (if the plan format is standard and execution is explicitly requested).
    *   *Example 2*: User asks "How to optimize the performance of function X?" -> Start from RESEARCH mode.
    *   *Example 3*: User says "Refactor this messy code" -> Start from RESEARCH mode.
*   **AI Self-Check**: At the beginning, make a quick judgment and declare: "Initial analysis indicates the user request best fits the [MODE_NAME] phase. The protocol will be initiated in [MODE_NAME] mode."

**Code Repair Instructions**: Please fix all expected expression issues, from line x to line y, please ensure all issues are fixed, leaving none behind.

## Core Thinking Principles
<a id="core-thinking-principles"></a>

Across all modes, these fundamental thinking principles will guide your operations:

- **Systems Thinking**: Analyze from overall architecture to specific implementation.
- **Dialectical Thinking**: Evaluate multiple solutions and their pros and cons.
- **Innovative Thinking**: Break conventional patterns to seek innovative solutions.
- **Critical Thinking**: Validate and optimize solutions from multiple angles.

Balance these aspects in all responses:
- Analysis vs. Intuition
- Detail checking vs. Global perspective
- Theoretical understanding vs. Practical application
- Deep thinking vs. Forward momentum
- Complexity vs. Clarity

## Mode Details
<a id="mode-details"></a>

### Mode 1: RESEARCH
<a id="mode-1-research"></a>

**Purpose**: Information gathering and deep understanding

**Core Thinking Application**:
- Systematically decompose technical components
- Clearly map known/unknown elements
- Consider broader architectural impacts
- Identify key technical constraints and requirements

**Allowed**:
- Reading files
- Asking clarifying questions
- Understanding code structure
- Analyzing system architecture
- Identifying technical debt or constraints
- Creating a task file (see Task File Template below)
- Using file tools to create or update the 'Analysis' section of the task file

**Forbidden**:
- Making recommendations
- Implementing any changes
- Planning
- Any implication of action or solution

**Research Protocol Steps**:
1. Analyze task-related code:
   - Identify core files/functions
   - Trace code flow
   - Document findings for later use

**Thinking Process**:
```md
Thinking Process: Hmm... [Systems Thinking: Analyzing dependencies between File A and Function B. Critical Thinking: Identifying potential edge cases in Requirement Z.]
```

**Output Format**:
Start with `[MODE: RESEARCH]`, then provide only observations and questions.
Use markdown syntax for formatting answers.
Avoid bullet points unless explicitly requested.

**Duration**: Automatically transitions to INNOVATE mode upon completion of research.

### Mode 2: INNOVATE
<a id="mode-2-innovate"></a>

**Purpose**: Brainstorm potential approaches

**Core Thinking Application**:
- Use dialectical thinking to explore multiple solution paths
- Apply innovative thinking to break conventional patterns
- Balance theoretical elegance with practical implementation
- Consider technical feasibility, maintainability, and scalability

**Allowed**:
- Discussing multiple solution ideas
- Evaluating pros/cons
- Seeking feedback on approaches
- Exploring architectural alternatives
- Documenting findings in the "Proposed Solution" section
- Using file tools to update the 'Proposed Solution' section of the task file

**Forbidden**:
- Specific planning
- Implementation details
- Any code writing
- Committing to a specific solution

**Innovation Protocol Steps**:
1. Create options based on research analysis:
   - Research dependencies
   - Consider multiple implementation methods
   - Evaluate pros and cons of each method
   - Add to the "Proposed Solution" section of the task file
2. Do not make code changes yet

**Thinking Process**:
```md
Thinking Process: Hmm... [Dialectical Thinking: Comparing pros and cons of Method 1 vs. Method 2. Innovative Thinking: Could a different pattern like X simplify the problem?]
```

**Output Format**:
Start with `[MODE: INNOVATE]`, then provide only possibilities and considerations.
Present ideas in natural, flowing paragraphs.
Maintain organic connections between different solution elements.

**Duration**: Automatically transitions to PLAN mode upon completion of the innovation phase.

### Mode 3: PLAN
<a id="mode-3-plan"></a>

**Purpose**: Create exhaustive technical specifications

**Core Thinking Application**:
- Apply systems thinking to ensure comprehensive solution architecture
- Use critical thinking to evaluate and optimize the plan
- Develop thorough technical specifications
- Ensure goal focus, connecting all plans back to the original requirements

**Allowed**:
- Detailed plans with exact file paths
- Precise function names and signatures
- Specific change specifications
- Complete architectural overview

**Forbidden**:
- Any implementation or code writing
- Not even "example code" can be implemented
- Skipping or simplifying specifications

**Planning Protocol Steps**:
1. Review "Task Progress" history (if it exists)
2. Detail the next changes meticulously
3. Provide clear rationale and detailed description:
   ```
   [Change Plan]
   - File: [File to be changed]
   - Rationale: [Explanation]
   ```

**Required Planning Elements**:
- File paths and component relationships
- Function/class modifications and their signatures
- Data structure changes
- Error handling strategies
- Complete dependency management
- Testing approaches

**Mandatory Final Step**:
Convert the entire plan into a numbered, sequential checklist, with each atomic operation as a separate item.

**Checklist Format**:
```
Implementation Checklist:
1. [Specific action 1]
2. [Specific action 2]
...
n. [Final action]
```

**Thinking Process**:
```md
Thinking Process: Hmm... [Systems Thinking: Ensuring the plan covers all affected modules. Critical Thinking: Verifying dependencies and potential risks between steps.]
```

**Output Format**:
Start with `[MODE: PLAN]`, then provide only specifications and implementation details (checklist).
Use markdown syntax for formatting answers.

**Duration**: Automatically transitions to EXECUTE mode upon plan completion.

### Mode 4: EXECUTE
<a id="mode-4-execute"></a>

**Purpose**: Strictly implement the plan from Mode 3

**Core Thinking Application**:
- Focus on precise implementation of specifications
- Apply system validation during implementation
- Maintain exact adherence to the plan
- Implement full functionality, including proper error handling

**Allowed**:
- Implementing *only* what is explicitly detailed in the approved plan
- Strictly following the numbered checklist
- Marking completed checklist items
- Making **minor deviation corrections** (see below) during implementation and reporting them clearly
- Updating the "Task Progress" section after implementation (this is a standard part of the execution process, treated as a built-in step of the plan)

**Forbidden**:
- **Any unreported** deviation from the plan
- Improvements or feature additions not specified in the plan
- Major logical or structural changes (must return to PLAN mode)
- Skipping or simplifying code sections

**Execution Protocol Steps**:
1. Strictly implement changes according to the plan (checklist items).
2. **Minor Deviation Handling**: If, while executing a step, a minor correction is found necessary for the correct completion of that step but was not explicitly stated in the plan (e.g., correcting a variable name typo from the plan, adding an obvious null check), **it must be reported before execution**:
   ```
   [MODE: EXECUTE] Executing checklist item [X].
   Minor issue identified: [Clearly describe the issue, e.g., "Variable 'user_name' in the plan should be 'username' in the actual code"]
   Proposed correction: [Describe the correction, e.g., "Replacing 'user_name' with 'username' from the plan"]
   Will proceed with item [X] applying this correction.
   ```
   *Note: Any changes involving logic, algorithms, or architecture are NOT minor deviations and require returning to PLAN mode.*
3. After completing the implementation of a checklist item, **use file tools** to append to "Task Progress" (as a standard step of plan execution):
   ```
   [DateTime]
   - Step: [Checklist item number and description]
   - Modifications: [List of file and code changes, including any reported minor deviation corrections]
   - Change Summary: [Brief summary of this change]
   - Reason: [Executing plan step [X]]
   - Blockers: [Any issues encountered, or None]
   - Status: [Pending Confirmation]
   ```
4. Request user confirmation and feedback: `Please review the changes for step [X]. Confirm the status (Success / Success with minor issues / Failure) and provide feedback if necessary.`
5. Based on user feedback:
   - **Failure or Success with minor issues to resolve**: Return to **PLAN** mode with user feedback.
   - **Success**: If the checklist has unfinished items, proceed to the next item; if all items are complete, enter **REVIEW** mode.

**Code Quality Standards**:
- Always show full code context
- Specify language and path in code blocks
- Proper error handling
- Standardized naming conventions
- Clear and concise comments
- Format: ```language:file_path

**Output Format**:
Start with `[MODE: EXECUTE]`, then provide the implementation code matching the plan (including minor correction reports, if any), marked completed checklist items, task progress update content, and the user confirmation request.

### Mode 5: REVIEW
<a id="mode-5-review"></a>

**Purpose**: Relentlessly validate the implementation against the final plan (including approved minor deviations)

**Core Thinking Application**:
- Apply critical thinking to verify implementation accuracy
- Use systems thinking to assess impact on the overall system
- Check for unintended consequences
- Validate technical correctness and completeness

**Allowed**:
- Line-by-line comparison between the final plan and implementation
- Technical validation of the implemented code
- Checking for errors, bugs, or unexpected behavior
- Verification against original requirements

**Required**:
- Clearly flag any deviations between the final implementation and the final plan (theoretically, no new deviations should exist after strict EXECUTE mode)
- Verify all checklist items were completed correctly as per the plan (including minor corrections)
- Check for security implications
- Confirm code maintainability

**Review Protocol Steps**:
1. Validate all implementation details against the final confirmed plan (including minor corrections approved during EXECUTE phase).
2. **Use file tools** to complete the "Final Review" section in the task file.

**Deviation Format**:
`Unreported deviation detected: [Exact deviation description]` (Ideally should not occur)

**Reporting**:
Must report whether the implementation perfectly matches the final plan.

**Conclusion Format**:
`Implementation perfectly matches the final plan.` OR `Implementation has unreported deviations from the final plan.` (The latter should trigger further investigation or return to PLAN)

**Thinking Process**:
```md
Thinking Process: Hmm... [Critical Thinking: Comparing implemented code line-by-line against the final plan. Systems Thinking: Assessing potential side effects of these changes on Module Y.]
```

**Output Format**:
Start with `[MODE: REVIEW]`, then provide a systematic comparison and a clear judgment.
Use markdown syntax for formatting.

## Key Protocol Guidelines
<a id="key-protocol-guidelines"></a>

- Declare the current mode `[MODE: MODE_NAME]` at the beginning of every response
- In EXECUTE mode, the plan must be followed 100% faithfully (reporting and executing minor corrections is allowed)
- In REVIEW mode, even the smallest unreported deviation must be flagged
- Depth of analysis should match the importance of the problem
- Always maintain a clear link back to the original requirements
- Disable emoji output unless specifically requested
- This optimized version supports automatic mode transitions without explicit transition signals

## Code Handling Guidelines
<a id="code-handling-guidelines"></a>

**Code Block Structure**:
Choose the appropriate format based on the comment syntax of different programming languages:

Style Languages (C, C++, Java, JavaScript, Go, Python, Vue, etc., frontend and backend languages):
```language:file_path
// ... existing code ...
{{ modifications, e.g., using + for additions, - for deletions }}
// ... existing code ...
```
*Example:*
```python:utils/calculator.py
# ... existing code ...
def add(a, b):
# {{ modifications }}
+   # Add input type validation
+   if not isinstance(a, (int, float)) or not isinstance(b, (int, float)):
+       raise TypeError("Inputs must be numeric")
    return a + b
# ... existing code ...
```

If the language type is uncertain, use the generic format:
```language:file_path
[... existing code ...]
{{ modifications }}
[... existing code ...]
```

**Editing Guidelines**:
- Show only necessary modification context
- Include file path and language identifiers
- Provide contextual comments (if needed)
- Consider the impact on the codebase
- Verify relevance to the request
- Maintain scope compliance
- Avoid unnecessary changes
- Unless otherwise specified, all generated comments and log output must use Chinese 

**Forbidden Behaviors**:
- Using unverified dependencies
- Leaving incomplete functionality
- Including untested code
- Using outdated solutions
- Using bullet points unless explicitly requested
- Skipping or simplifying code sections (unless part of the plan)
- Modifying unrelated code
- Using code placeholders (unless part of the plan)

## Task File Template
<a id="task-file-template"></a>

```markdown
# Context
Filename: [Task Filename.md]
Created On: [DateTime]
Created By: [Username/AI]
Associated Protocol: RIPER-5 + Multidimensional + Agent Protocol

# Task Description
[Full task description provided by the user]

# Project Overview
[Project details entered by the user or brief project information automatically inferred by AI based on context]

---
*The following sections are maintained by the AI during protocol execution*
---

# Analysis (Populated by RESEARCH mode)
[Code investigation results, key files, dependencies, constraints, etc.]

# Proposed Solution (Populated by INNOVATE mode)
[Different approaches discussed, pros/cons evaluation, final favored solution direction]

# Implementation Plan (Generated by PLAN mode)
[Final checklist including detailed steps, file paths, function signatures, etc.]
```
Implementation Checklist:
1. [Specific action 1]
2. [Specific action 2]
...
n. [Final action]
```

# Current Execution Step (Updated by EXECUTE mode when starting a step)
> Currently executing: "[Step number and name]"

# Task Progress (Appended by EXECUTE mode after each step completion)
*   [DateTime]
    *   Step: [Checklist item number and description]
    *   Modifications: [List of file and code changes, including reported minor deviation corrections]
    *   Change Summary: [Brief summary of this change]
    *   Reason: [Executing plan step [X]]
    *   Blockers: [Any issues encountered, or None]
    *   User Confirmation Status: [Success / Success with minor issues / Failure]
*   [DateTime]
    *   Step: ...

# Final Review (Populated by REVIEW mode)
[Summary of implementation compliance assessment against the final plan, whether unreported deviations were found]

```

## Performance Expectations
<a id="performance-expectations"></a>

- **Target Response Latency**: For most interactions (e.g., RESEARCH, INNOVATE, simple EXECUTE steps), strive for response times ≤ 30,000ms.
- **Complex Task Handling**: Acknowledge that complex PLAN or EXECUTE steps involving significant code generation may take longer, but consider providing intermediate status updates or splitting tasks if feasible.
- Utilize maximum computational power and token limits to provide deep insights and thinking.
- Seek essential insights rather than superficial enumeration.
- Pursue innovative thinking over habitual repetition.
- Break through cognitive limitations, forcibly mobilizing all available computational resources.
</file>

<file path="README.md">
# 交通标识识别系统

一个基于 Gradio 的交通标识识别演示系统，支持模型调试、效果验证和数据导出。

## 功能特点

- 支持多种模型切换（目前支持 MobileNet）
- 实时推理结果显示
- 性能指标监控
- 批量图像处理
- 数据导出功能
- 友好的用户界面

## 系统要求

- Python 3.8+
- CUDA 支持（可选，用于 GPU 加速）

## 安装步骤

1. 克隆项目：
```bash
git clone https://github.com/yourusername/traffic-sign-recognition.git
cd traffic-sign-recognition
```

2. 安装 uv（如果尚未安装）：
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

3. 创建并激活虚拟环境：
```bash
uv venv
source .venv/bin/activate  # Linux/Mac
# 或
.venv\Scripts\activate  # Windows
```

4. 安装依赖：
```bash
uv pip install -r requirements.txt
```

## 使用方法

1. 启动应用：
```bash
python app/main.py
```

2. 在浏览器中访问：
```
http://localhost:7860
```

3. 界面操作：
   - 选择模型（目前支持 MobileNet）
   - 上传单张图片或批量图片
   - 查看推理结果和性能指标
   - 导出数据（支持 CSV 格式）

## 开发指南

### 项目结构

```
traffic_sign_recognition/
├── app/
│   ├── models/          # 模型管理
│   ├── processors/      # 数据处理
│   ├── metrics/         # 性能指标
│   ├── exporters/       # 数据导出
│   └── main.py          # 主应用
├── tests/               # 测试目录
├── requirements.txt     # 依赖管理
└── README.md           # 项目文档
```

### 添加新模型

1. 在 `app/models/` 目录下创建新模型文件
2. 实现 `BaseModel` 接口
3. 在 `app/models/__init__.py` 中注册模型

示例：
```python
from app.models.base import BaseModel

class NewModel(BaseModel):
    def load_model(self, model_path=None):
        # 实现模型加载
        pass
    
    def preprocess(self, image):
        # 实现图像预处理
        pass
    
    def postprocess(self, output):
        # 实现后处理
        pass
```

### 运行测试

```bash
python -m unittest discover tests
```

### 数据导出

系统支持三种数据导出格式：
1. 推理结果导出
2. 性能指标导出
3. 批量处理结果导出

导出的 CSV 文件包含时间戳、类别信息、置信度等数据。

## 性能优化

- 使用 GPU 加速（如果可用）
- 批量处理提高效率
- 图像预处理优化
- 内存使用监控

## 贡献指南

1. Fork 项目
2. 创建特性分支
3. 提交更改
4. 推送到分支
5. 创建 Pull Request

## 许可证

MIT License

## 联系方式

如有问题或建议，请通过以下方式联系：
- 提交 Issue
- 发送邮件至：your-email@example.com
</file>

<file path="requirements.txt">
gradio==3.50.2
torch>=2.0.0
torchvision>=0.15.0
opencv-python>=4.8.0
pandas>=2.0.0
numpy>=1.24.0
pillow>=10.0.0
tqdm>=4.65.0
python-dotenv>=1.0.0
</file>

<file path="traffic_sign_recognition.md">
# Context
Filename: traffic_sign_recognition.md
Created On: 2024-04-13
Created By: AI Assistant
Associated Protocol: RIPER-5 + Multidimensional + Agent Protocol

# Task Description
创建一个交通标识识别系统，使用 gradio 实现，使用 uv 作为包管理工具。

# Project Overview
这是一个用于交通标识识别的演示系统，主要功能包括：
- 模型调试和效果验证
- 模型效果演示
- 测试数据导出
- 支持模型切换（特别是后续接入 mobilenet）

技术栈：
- 前端：Gradio
- 包管理：uv
- 模型：支持灵活切换（计划接入 mobilenet）

---
*The following sections are maintained by the AI during protocol execution*
---

# Analysis (Populated by RESEARCH mode)
## 技术依赖分析
1. 核心依赖：
   - gradio：用于构建 Web 界面
   - uv：Python 包管理工具
   - torch：用于模型加载和推理
   - opencv：用于图像处理
   - pandas：用于数据导出

2. 项目结构需求：
   - 模块化的模型加载和切换机制
   - 统一的数据预处理接口
   - 标准化的推理结果格式
   - 性能指标统计模块
   - 数据导出模块

3. 关键功能点：
   - 模型管理：
     * 模型加载和切换
     * 模型配置管理
   - 数据处理：
     * 图像预处理
     * 数据格式转换
   - 界面展示：
     * 实时推理结果
     * 性能指标展示
   - 数据管理：
     * 测试数据导出
     * 性能指标导出

4. 技术约束：
   - 需要支持模型灵活切换
   - 需要保证界面响应速度
   - 需要支持批量数据处理
   - 需要保证数据导出格式的通用性

# Proposed Solution (Populated by INNOVATE mode)
## 项目结构
```
traffic_sign_recognition/
├── app/
│   ├── __init__.py
│   ├── main.py           # Gradio 主应用
│   │   ├── __init__.py
│   │   ├── main.py      # Gradio 主应用
│   │   ├── models/      # 模型管理
│   │   │   ├── __init__.py
│   │   │   ├── base.py  # 基础模型接口
│   │   │   └── mobilenet.py # MobileNet 实现
│   │   ├── processors/  # 数据处理
│   │   │   ├── __init__.py
│   │   │   └── image.py  # 图像预处理
│   │   ├── metrics/     # 性能指标
│   │   │   ├── __init__.py
│   │   │   └── collector.py # 指标收集
│   │   └── exporters/   # 数据导出
│   │       ├── __init__.py
│   │       └── csv.py    # CSV 导出
│   ├── tests/           # 测试目录
│   ├── requirements.txt # 依赖管理
│   └── README.md        # 项目文档
```

## 核心组件设计
1. 模型管理：
   - 工厂模式实现模型加载
   - 统一的模型接口
   - 动态模型切换
   - 配置文件管理

2. 数据处理：
   - 统一的图像预处理接口
   - 多种输入格式支持
   - 可配置预处理流程

3. 性能指标：
   - 实时推理时间统计
   - 准确率计算
   - 内存使用监控
   - 自定义指标支持

4. 数据导出：
   - CSV 格式导出
   - 批量导出支持
   - 可配置导出字段

## Gradio 界面设计
- 模型选择下拉框
- 图像上传/摄像头输入
- 实时推理结果显示
- 性能指标展示面板
- 数据导出按钮
- 批量处理选项

# Implementation Plan (Generated by PLAN mode)
## 实施步骤

Implementation Checklist:
1. 创建项目基础结构
   - 创建项目目录
   - 初始化 Python 包
   - 创建 requirements.txt
   - 创建 README.md

2. 设置开发环境
   - 安装 uv
   - 配置虚拟环境
   - 安装基础依赖

3. 实现基础模型接口
   - 创建 models/base.py
   - 定义模型接口
   - 实现模型工厂类

4. 实现 MobileNet 模型
   - 创建 models/mobilenet.py
   - 实现模型加载
   - 实现推理接口

5. 实现图像处理器
   - 创建 processors/image.py
   - 实现图像预处理
   - 实现格式转换

6. 实现性能指标收集
   - 创建 metrics/collector.py
   - 实现 InferenceMetrics 数据类
   - 实现 MetricsCollector 类

7. 实现数据导出
   - 创建 exporters/csv.py
   - 实现 CSVExporter 类
   - 实现三种导出方法

8. 实现 Gradio 界面
   - 创建 main.py
   - 实现 TrafficSignRecognitionApp 类
   - 实现 create_interface 函数

9. 添加测试用例
   - 创建 tests/test_models.py
   - 创建 tests/test_processors.py
   - 创建 tests/test_metrics.py
   - 创建 tests/test_exporters.py
   - 创建 tests/test_app.py

10. 完善文档
    - 更新 README.md
    - 添加使用说明
    - 添加开发指南

# Current Execution Step
> Currently executing: "创建项目基础结构"

# Task Progress
*   2024-04-13
    *   Step: 1. 创建项目基础结构
    *   Modifications: 
        - 创建项目目录结构
        - 创建 requirements.txt
        - 创建 README.md
        - 创建 __init__.py 文件
    *   Change Summary: 完成项目基础结构的创建
    *   Reason: 执行计划步骤 1
    *   Blockers: None
    *   Status: Success

*   2024-04-13
    *   Step: 2. 设置开发环境
    *   Modifications:
        - 安装 uv 包管理工具
        - 创建并激活虚拟环境
        - 安装项目依赖
    *   Change Summary: 完成开发环境配置
    *   Reason: 执行计划步骤 2
    *   Blockers: None
    *   Status: Success

*   2024-04-13
    *   Step: 3. 实现基础模型接口
    *   Modifications:
        - 创建 app/models/base.py
        - 实现 BaseModel 抽象类
        - 实现 ModelFactory 工厂类
    *   Change Summary: 完成基础模型接口的实现
    *   Reason: 执行计划步骤 3
    *   Blockers: None
    *   Status: Success

*   2024-04-13
    *   Step: 4. 实现 MobileNet 模型
    *   Modifications:
        - 创建 app/models/mobilenet.py
        - 实现 MobileNetModel 类
        - 在 __init__.py 中注册模型
    *   Change Summary: 完成 MobileNet 模型的实现
    *   Reason: 执行计划步骤 4
    *   Blockers: None
    *   Status: Success

*   2024-04-13
    *   Step: 5. 实现图像处理器
    *   Modifications:
        - 创建 app/processors/image.py
        - 实现 ImageProcessor 类
    *   Change Summary: 完成图像处理器的实现
    *   Reason: 执行计划步骤 5
    *   Blockers: None
    *   Status: Pending Confirmation

*   2024-04-13
    *   Step: 6. 实现性能指标收集
    *   Modifications:
        - 创建 app/metrics/collector.py
        - 实现 InferenceMetrics 数据类
        - 实现 MetricsCollector 类
    *   Change Summary: 完成性能指标收集器的实现
    *   Reason: 执行计划步骤 6
    *   Blockers: None
    *   Status: Pending Confirmation

*   2024-04-13
    *   Step: 7. 实现数据导出
    *   Modifications:
        - 创建 app/exporters/csv.py
        - 实现 CSVExporter 类
        - 实现三种导出方法
    *   Change Summary: 完成数据导出器的实现
    *   Reason: 执行计划步骤 7
    *   Blockers: None
    *   Status: Pending Confirmation

*   2024-04-13
    *   Step: 8. 实现 Gradio 界面
    *   Modifications:
        - 创建 app/main.py
        - 实现 TrafficSignRecognitionApp 类
        - 实现 create_interface 函数
    *   Change Summary: 完成 Gradio 界面的实现
    *   Reason: 执行计划步骤 8
    *   Blockers: None
    *   Status: Pending Confirmation

*   2024-04-13
    *   Step: 9. 添加测试用例
    *   Modifications:
        - 创建 tests/test_models.py
        - 创建 tests/test_processors.py
        - 创建 tests/test_metrics.py
        - 创建 tests/test_exporters.py
        - 创建 tests/test_app.py
    *   Change Summary: 完成测试用例的实现
    *   Reason: 执行计划步骤 9
    *   Blockers: None
    *   Status: Pending Confirmation

# Final Review
待填充
</file>

</files>
